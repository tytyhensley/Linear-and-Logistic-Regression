{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#       REPORT for Part A: Linear Regression\n",
    "\n",
    "The boston house prices datasets is used to predict the prices of new homes on the market. With this machine language structure, linear and ridge regression were implemeneted to get a prediction. The structure utilizes both of these models with exhaustive testing of the dataset through a k-fold validaton of k spits. For each unique k split, only one fold is the tester and the other trainers. These unique data sets are used on the liner and reidge regressions for each iteration of k different folds you will get. For the ridge regression, we also iterate based on the different values of lambda(alpha) that is used for the k-fold validation. The total error averages for each linear regression and ridge regression for each alpha are determined with the overall minimum used as the model for predicting housing prices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the various needed python libraries and boston data set that will used to train/test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_boston######\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bostonh_data = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Assigning references to different areas of the dataset. X holds the data and y holds its targets. The y variable is also reshaped into a column vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = bostonh_data.data\n",
    "y = bostonh_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
    "y = y.reshape(X.shape[0], 1)\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', bostonh_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "# Observing the first 2 rows of the data\n",
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a polynomial of degree 2 and fitting the X data to that polynomial and printing the new shape of both X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
    "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
    "# Simply copy Y into Y_2 \n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Defining the functions of linear and ridge regression\n",
    "\n",
    "Linear regression functon using normal function gets theat and avoids using gradients . This is done by multiplying a matrix X with its tranpose, getin the inverse and multiplying it by its orginal. \n",
    "\n",
    "Ridge regression involves the use of gradient to get theta which entails multiplying a identity matrix the same size as X by a set of alphas ranging from 10^1, 10^1.5,....,10^7. The scaled matrix is then added to the product of X and its transpose. This is then inverted and multplied with the oproduct of X and it transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_linear_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "#Using the equation from the Project Lab slides\n",
    "#  w = (x^T X)^-1 (X^T y)\n",
    "def get_coeff_linear_normaleq(X_train, y_train):\n",
    "    w = np.matmul(np.linalg.pinv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, y_train))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "#Using the equation from the Project Lab slides\n",
    "#w = ((x^T X)+ aI)^-1(X^T y)\n",
    "#term1 = (x^T X)\n",
    "#term1 = (aI)\n",
    "#term1 = (x^T y)\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    term1 = np.matmul(X_train.T, X_train)\n",
    "    term2 = np.identity(X_train.shape[1])*alpha\n",
    "    term3 = np.matmul(X_train.T, y_train)\n",
    "    w = np.matmul(np.linalg.pinv(term1+term2), term3)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Evaluating the errors \n",
    "\n",
    "The errors of the predicted values for housing prices are obtaineed from running model with training data and getting the difference between the predicted values and the test data. The predicted values are obtained from  multplying X with either regression. This difeerence is squared and and all the values are averaged. \n",
    "\n",
    "###    Implementing the k fold cross validation \n",
    "\n",
    "Which computes the linear or ridge regression of the data set for each varying k fold to get an exhaustive testing of the model. It splits the data sets into a set number of k folds where only one fold is the tester at a time, this is changed with each iteration. The data is centered and scaled, and depending on choice whichever regression is used to obtain theta and its error calculated for each iteration. This error is then taken into an overall average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_err_ridge function.\n",
    "# Return the train_error and test_error values\n",
    "# Using the equation from the lecture 3 slides \n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "#     pred_train=prediction using w and X_tran+np.mean(y_train)\n",
    "#     pred_test=prediction using w and X_test\n",
    "#     remember to add the mean back\n",
    "#     train_error=...\n",
    "#     test_error=...\n",
    "\n",
    "    #getting a matrix that consists of all the n predicted values of y train and y test\n",
    "    pred_train = np.matmul(X_train, w) + np.mean(y_train) \n",
    "    pred_test = np.matmul(X_test, w) + np.mean(y_train)\n",
    "    \n",
    "    #getting matrces of the diffrence between all n values of y test and y train and their respective predicted values squared\n",
    "    train_error = (y_train - pred_train)**2\n",
    "    test_error = (y_test - pred_test) **2\n",
    "    \n",
    "    #getting the mean square error from the summation of all the diferecnes in value divided by the total number of elements\n",
    "    train_error = np.sum(train_error)/y_train.shape[0]\n",
    "    test_error = np.sum(test_error)/y_test.shape[0]\n",
    "    \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish writting the k_fold_cross_validation function. \n",
    "# Returns the average training error and average test error from the k-fold cross validation\n",
    "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha, linrid):\n",
    "    min_train_error = 9999\n",
    "    min_test_error = 9999\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        y_train_mean = np.mean(y_train)\n",
    "        y_train = y_train - y_train_mean\n",
    "        y_test = y_test - y_train_mean\n",
    "        \n",
    "        # Scaling the data matrix\n",
    "        scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Determine the training error and the test error\n",
    "        \n",
    "        #Derterming whether to use linear or ridge regression, linear if true, ridge otherwise\n",
    "        if linrid == True:\n",
    "            w = get_coeff_linear_normaleq(X_train, y_train)\n",
    "        else:\n",
    "            w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "     \n",
    "        min_train_error, min_test_error = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        \n",
    "        #getting the total performance for the regression determinded by the k fold validation\n",
    "        total_E_val_test += min_test_error\n",
    "        total_E_val_train += min_train_error\n",
    "        \n",
    "    #averaging the performance of the regressioon for the overall k folds\n",
    "    total_E_val_test = total_E_val_test/k\n",
    "    total_E_val_train = total_E_val_train/k\n",
    "    \n",
    "    return  total_E_val_test, total_E_val_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Finding the best lambda\n",
    "\n",
    "Using any number for k to represent folds, the variation was conducted for each alpha to get its total average error and then the alpha with the minimm error was declared the best one. This model used th eoriginal X and y data\n",
    "\n",
    "The model was then ran with the X^2 polynomial data and the best alpha was found for that set too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########LINEAR REGRESSION############\n",
      "Total test error:  23.773613682085497\n",
      "Total train error:  21.885480719855178\n",
      "_____________________________________________________\n",
      "\n",
      "\n",
      "########RIDGE REGRESSION############\n",
      "_____________________________________________________\n",
      "Alpha:  10.0\n",
      "Test error:  23.7890592502954\n",
      "Train error:  21.959504864528657\n",
      "_____________________________________________________\n",
      "Alpha:  31.622776601683793\n",
      "Test error:  24.03950189103493\n",
      "Train error:  22.30713860896172\n",
      "_____________________________________________________\n",
      "Alpha:  100.0\n",
      "Test error:  25.130684396920145\n",
      "Train error:  23.610271468580894\n",
      "_____________________________________________________\n",
      "Alpha:  316.22776601683796\n",
      "Test error:  28.88537110197723\n",
      "Train error:  27.674191548972622\n",
      "_____________________________________________________\n",
      "Alpha:  1000.0\n",
      "Test error:  38.41394839825479\n",
      "Train error:  37.49236322124653\n",
      "_____________________________________________________\n",
      "Alpha:  3162.2776601683795\n",
      "Test error:  53.371454743833844\n",
      "Train error:  52.678808646871296\n",
      "_____________________________________________________\n",
      "Alpha:  10000.0\n",
      "Test error:  68.76713110658135\n",
      "Train error:  68.21224337879485\n",
      "_____________________________________________________\n",
      "Alpha:  31622.776601683792\n",
      "Test error:  78.52532504712632\n",
      "Train error:  78.03123929215008\n",
      "_____________________________________________________\n",
      "Alpha:  100000.0\n",
      "Test error:  82.69972606847186\n",
      "Train error:  82.22787413977098\n",
      "_____________________________________________________\n",
      "Alpha:  316227.7660168379\n",
      "Test error:  84.17096844556905\n",
      "Train error:  83.70653727695216\n",
      "_____________________________________________________\n",
      "Alpha:  1000000.0\n",
      "Test error:  84.65318473002634\n",
      "Train error:  84.19114318529442\n",
      "_____________________________________________________\n",
      "Alpha:  3162277.6601683795\n",
      "Test error:  84.80743730657872\n",
      "Train error:  84.3461558678652\n",
      "_____________________________________________________\n",
      "Alpha:  10000000.0\n",
      "Test error:  84.85639464497555\n",
      "Train error:  84.39535402167316\n",
      "\n",
      "\n",
      "\n",
      "Best alpha:  10.0\n",
      "Minimum test error:  23.7890592502954\n",
      "Minimum train error:  21.959504864528657\n"
     ]
    }
   ],
   "source": [
    "alpha = np.logspace(1, 7, num=13)\n",
    "k = 10\n",
    "min_train_error = 9999\n",
    "min_test_error = 9999\n",
    "\n",
    "total_error_test, total_error_train = k_fold_cross_validation(k, X, y, alpha, True)\n",
    "print(\"########LINEAR REGRESSION############\")\n",
    "print (\"Total test error: \", total_error_test)\n",
    "print (\"Total train error: \", total_error_train)\n",
    "print(\"_____________________________________________________\\n\\n\") \n",
    "\n",
    "print(\"########RIDGE REGRESSION############\")\n",
    "for x in range(0, 13):\n",
    "    total_error_test, total_error_train = k_fold_cross_validation(k, X, y, alpha[x], False)\n",
    "    print(\"_____________________________________________________\")\n",
    "    print(\"Alpha: \", alpha[x])\n",
    "    print(\"Test error: \", total_error_test)\n",
    "    print(\"Train error: \", total_error_train)\n",
    "    \n",
    "    #determines the overall minimum alpha throughout the k-fold validation\n",
    "    if total_error_test < min_test_error:\n",
    "        min_test_error = total_error_test\n",
    "        min_train_error = total_error_train\n",
    "        min_alpha = alpha[x]\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print (\"Best alpha: \", min_alpha)\n",
    "print (\"Minimum test error: \", min_test_error)\n",
    "print (\"Minimum train error: \", min_train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########LINEAR REGRESSION############\n",
      "Total test error:  12.688085050328374\n",
      "Total train error:  5.974831823338861\n",
      "_____________________________________________________\n",
      "\n",
      "\n",
      "########RIDGE REGRESSION############\n",
      "_____________________________________________________\n",
      "Alpha:  10.0\n",
      "Test error:  13.404801243846377\n",
      "Train error:  10.024584491538686\n",
      "_____________________________________________________\n",
      "Alpha:  31.622776601683793\n",
      "Test error:  15.715600046721931\n",
      "Train error:  12.617122959383908\n",
      "_____________________________________________________\n",
      "Alpha:  100.0\n",
      "Test error:  18.885442276702758\n",
      "Train error:  16.065010589998312\n",
      "_____________________________________________________\n",
      "Alpha:  316.22776601683796\n",
      "Test error:  21.91738957056631\n",
      "Train error:  19.52011837138036\n",
      "_____________________________________________________\n",
      "Alpha:  1000.0\n",
      "Test error:  25.771733205654787\n",
      "Train error:  23.900948880551617\n",
      "_____________________________________________________\n",
      "Alpha:  3162.2776601683795\n",
      "Test error:  33.62492818488758\n",
      "Train error:  32.22775583017845\n",
      "_____________________________________________________\n",
      "Alpha:  10000.0\n",
      "Test error:  46.60007863756121\n",
      "Train error:  45.59250889298879\n",
      "_____________________________________________________\n",
      "Alpha:  31622.776601683792\n",
      "Test error:  61.53667450021012\n",
      "Train error:  60.83379534967567\n",
      "_____________________________________________________\n",
      "Alpha:  100000.0\n",
      "Test error:  74.07358867091052\n",
      "Train error:  73.52670544540273\n",
      "_____________________________________________________\n",
      "Alpha:  316227.7660168379\n",
      "Test error:  80.86156819713479\n",
      "Train error:  80.37260249753123\n",
      "_____________________________________________________\n",
      "Alpha:  1000000.0\n",
      "Test error:  83.53370183749954\n",
      "Train error:  83.0638333019454\n",
      "_____________________________________________________\n",
      "Alpha:  3162277.6601683795\n",
      "Test error:  84.44554000135025\n",
      "Train error:  83.98177717543663\n",
      "_____________________________________________________\n",
      "Alpha:  10000000.0\n",
      "Test error:  84.74114391202718\n",
      "Train error:  84.27931799818758\n",
      "\n",
      "\n",
      "\n",
      "Best alpha:  10.0\n",
      "Minimum test error:  13.404801243846377\n",
      "Minimum train error:  10.024584491538686\n"
     ]
    }
   ],
   "source": [
    "total_error_test, total_error_train = k_fold_cross_validation(k, X_2, y_2, alpha, True)\n",
    "print(\"########LINEAR REGRESSION############\")\n",
    "print (\"Total test error: \", total_error_test)\n",
    "print (\"Total train error: \", total_error_train)\n",
    "print(\"_____________________________________________________\\n\\n\") \n",
    "\n",
    "print(\"########RIDGE REGRESSION############\")\n",
    "for x in range(0, 13):\n",
    "    total_error_test, total_error_train = k_fold_cross_validation(k, X_2, y_2, alpha[x], False)\n",
    "    print(\"_____________________________________________________\")\n",
    "    print(\"Alpha: \", alpha[x])\n",
    "    print(\"Test error: \", total_error_test)\n",
    "    print(\"Train error: \", total_error_train)\n",
    "    if total_error_test < min_test_error:\n",
    "        min_test_error = total_error_test\n",
    "        min_train_error = total_error_train\n",
    "        min_alpha = alpha[x]\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print (\"Best alpha: \", min_alpha)\n",
    "print (\"Minimum test error: \", min_test_error)\n",
    "print (\"Minimum train error: \", min_train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The best lamba is 10 when using the X data set transformed to fit a polynomial of degree 2 as it results in the lowest minimum erorr for both the teting and trainng data splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
