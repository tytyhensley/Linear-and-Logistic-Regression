{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   REPORT for Part B: Logistic Regression\n",
    "\n",
    "The breast cancer datasets is used to predict the likelihood of a woman getting breast cancer dependent on various criteria. With this machine language structure, a logistic regression was used to obtin a prediction. The structure utilizes various logistic fucntions from sigmoid, hypothesis to log likelihood. These are used to plot the points of the data into a bound real function that is defined for all real input values and has no non negative derivaives. this allows us to get only a positive (1) or a negtive(0) and nothig in between, which maks it easy to account for any false negatives and positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the standard libraries and breast cancer dataset\n",
    "\n",
    "The y value was reshaped in order to make a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to a python variable cancer\n",
    "# Store target to a variable called y\n",
    "# Store feature to a variable called X\n",
    "\n",
    "breast_data = load_breast_cancer()\n",
    "\n",
    "X = breast_data.data\n",
    "y = breast_data.target\n",
    "y = y.reshape(-1,1)# shapes y into a 569,1 matrix which helps with operations and calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 1)\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of data (X) and target (Y) values \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "#### Splitting the data into train and test before scaling the dataset and then caling it as gradient ascent will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split() function to split the dataset\n",
    "# Store the return value of previous step to X_train, X_test, y_train, y_test\n",
    "\n",
    "#using the train test split function to have a 75/25 split for the train and test subsets respectively and divided into X and y columns \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scaler of the dataset by using preprocessing.StandardScaler().fit()\n",
    "# Using this scale to scale the X_train and X_test using .transform()\n",
    "\n",
    "#dataset used was X and it correlates to the data in the breast \n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Print the shape of x_train and y_train \n",
    "# print(X_train.shape) # It should print (426, 30)\n",
    "# print(y_train.shape) # It should print (426,)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a column of ones to the  matrices $X_{train}$ and  $X_{test}$\n",
    "After adding a column of ones $X_{train}=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "\n",
    "Similarly for $X_{test}$\n",
    "\n",
    "Populating the first column of the X train dataset with ones in order to match the hypothesis fucntion that consists of the first w coefficient being multiplied by one at all times     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trainng data has dimensions:  (426, 31) . The testing data has dimensions:  (143, 31)\n",
      "[[ 1.         -0.4606494  -1.51921209 -0.52895827 -0.49132968 -1.5747797\n",
      "  -1.21976641 -1.01947749 -0.95551312 -0.71640584 -0.37884191 -0.27283509\n",
      "   0.49013027 -0.36765    -0.35401257  1.10534093 -0.86108372 -0.79277011\n",
      "  -0.61933335  0.97464049  0.3585407  -0.62632899 -1.54014551 -0.7068611\n",
      "  -0.59956596 -1.61844737 -1.2104119  -1.20825122 -1.33723288 -1.01450733\n",
      "  -0.73276768]\n",
      " [ 1.         -0.01459724 -0.85209193 -0.08180982 -0.12192181 -1.15730174\n",
      "  -0.92226774 -0.84653678 -0.56644964 -0.80546441 -0.9794459  -0.68456535\n",
      "  -1.04066819 -0.61931202 -0.47989779 -0.71264698 -0.90523397 -0.70640286\n",
      "  -0.16382774 -1.08691951 -0.63030571 -0.26629784 -1.04993928 -0.30316885\n",
      "  -0.32284398 -1.22897135 -0.9716658  -0.9889906  -0.48789496 -1.25396625\n",
      "  -0.90175967]]\n"
     ]
    }
   ],
   "source": [
    "# Append a column of ones to x_train \n",
    "\n",
    "# Create a column vector of ones by using np.ones and reshape\n",
    "# Append a column of ones in the beginning of x_train by using np.hstack\n",
    "\n",
    "ones = np.ones(X_train.shape[0])#creates a single row that consists of 426 ones\n",
    "ones = ones.reshape(X_train.shape[0],1)#reshapes that row into individual column vector\n",
    "\n",
    "\n",
    "X_train = np.hstack((ones, X_train))#appends the column vector to the X_train dataset, resulting in 31 columns and a 1 being added to the beginning of each row\n",
    "\n",
    "\n",
    "# Now do the same for the test data\n",
    "one = np.ones(X_test.shape[0])#creates a single row that consists of 143 ones\n",
    "one = one.reshape(X_test.shape[0],1)#reshapes that row into individual column vector\n",
    "\n",
    "\n",
    "X_test = np.hstack((one, X_test))#appends the column vector to the X_test dataset, resulting in 31 columns and a 1 being added to the beginning of each row\n",
    "\n",
    "# We can check that everything worked correctly by:\n",
    "# Printing out the new dimensions\n",
    "print(\"The trainng data has dimensions: \", X_train.shape, \". The testing data has dimensions: \", X_test.shape)\n",
    "# Looking at the first two rows of X_train to check everything worked as expected\n",
    "print(X_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# Printing the names of all the features\n",
    "print(breast_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Implemeting the sigmoid, hypothesis and log likelihood functions \n",
    "\n",
    "The sigmoid fuction consists of taking the column vector given and plotting unto the bounded fucntion which will result in getting points that are either positive or negative\n",
    "\n",
    "### Sigmoid($z$)\n",
    "The first function you will write is sigmoid($z$)\n",
    "\n",
    "sigmoid($z$) takes as input a column vector of real numbers, $z^T = [z_1, z_2, ..., z_{N'}]$, where $N'$ is the number of  examples\n",
    "\n",
    "It should produce as output a column vector $\\left[\\frac{1}{1+e^{-z_1}},\\frac{1}{1+e^{-z_2}},...,\\frac{1}{1+e^{-z_{N'}}}\\right]^T$\n",
    "\n",
    "The hypothesis fucntion includes making the X train data into a column verctor by multiplying it with w coefficients\n",
    "\n",
    "### Our hypothesis, $h({\\bf x})$\n",
    "The next  function to write is our hypothesis function. \n",
    "\n",
    "For example if our design matrix $X$ consists of single example $X=[1,x_1,x_2,\\ldots,x_d]$ and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, it returns $h({\\bf x})=\\frac{1}{1+e^{-\\left({w_{0}\\cdot 1 +w_1\\cdot x_1+\\cdots w_d\\cdot x_d}\\right)}}$\n",
    "\n",
    "If given a  matrix consisting of $N'$ examples \n",
    "$X=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "and  weights ${\\bf w}^T=[w_0,w_2,\\ldots, w_d]$, the function returns a column vector\n",
    "$[h({\\bf x}^{(1)}),h({\\bf x}^{(2)},\\ldots, h({\\bf x}^{(N')}]^T$\n",
    "\n",
    "The log likelihood fuction consists of determing the likelihood that someone will have breast cancer by using the hypothesis and sigmoid fucntions. \n",
    "\n",
    "### Log-Likelihood Function.\n",
    "Write the code to calculate the log likelihood function $\\ell({\\bf w})=\\frac{1}{m}\n",
    "\\sum_{i=1}^{N'}y^{(i)}\\ln(h({\\bf x}^{(i)})) +(1- y^{(i)})\\ln(1-h({\\bf x}^{(i)}))$ where m is the number of examples in the X train dataset\n",
    "\n",
    "The input is a matrix consisting of $N'$ examples $X=\\left[\\begin{matrix}\n",
    "1& x^{(1)}_1& x^{(1)}_2 &\\ldots& x^{(1)}_d\\\\\n",
    "1& x^{(2)}_1& x^{(2)}_2 &\\ldots& x^{(2)}_d\\\\\n",
    "\\vdots & \\vdots &\\vdots & & \\vdots \\\\\n",
    "1& x^{(N')}_1& x^{(N')}_2 &\\ldots& x^{(N')}_d\\\\\n",
    "\\end{matrix}\\right]$\n",
    "and a column vector ${\\bf y}^T=[y^{(1)},y^{(2)},\\dots,y^{(N')}]$ of labels for $X$.\n",
    "\n",
    "The output is $\\ell({\\bf w})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sigmoid function\n",
    "#sigmoid function taken from project 1 lab slides\n",
    "#sigmoid function = [1/1+e^-z_1.....1/1+e^-z_N']^T for each\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z)) #returns column vector where each row is populated with 1/1+e^-z_N' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w using np.zeros()\n",
    "#w = np.zeros((X_train.shape[1], 1))#creates a single row that consists of 31 zeros\n",
    "# w = w.reshape(X_train.shape[1],1)#reshapes that row into individual column vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the probability that a patient has cancer \n",
    "# Write the hypothesis function\n",
    "#hypothesis function taken from project 1 slides\n",
    "#hypithesis fucntion = [h(x)^1,....,h(x)^N'] where h(x) = [1/1+e^-(w^0.1,w^1.x^1....w^d.x^d)]\n",
    "\n",
    "def hypothesis(X , w):\n",
    "    v = np.matmul(X, w)# matrix multiplication that will result in a column vector with each cell containing the summation of w^0.1,w^1.x^1....w^d.x^d\n",
    "    return sigmoid(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the log likelihood function\n",
    "#log likelihood function taken from project 1 slides and cost function taken from lecture 003 slides\n",
    "#log likelihood function l(w) = [y^i.(ln(h(x^i)))+ 1-y^i.(ln(1-h(x^i)))]^1+ ...+ [y^i.(ln(h(x^i)))+ 1-y^i.(ln(1-h(x^i)))]^N\n",
    "def log_likelihood(X , y , w ):\n",
    "    m = X.shape[0]\n",
    "    h = hypothesis(X, w)#get h(x) of the function from the hypothesis function implemented above \n",
    "    \n",
    "    #w = populate each row of the column vector with y^i.(ln(h(x^i)))+ 1-y^i.(ln(1-h(x^i)))\n",
    "    \n",
    "    Lw = (1/m) * np.sum((y*np.log(h))+((1-y)*np.log(1-h))) #get the total summantion of each row to complete the log likehood function\n",
    "    return Lw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Finding the gradient\n",
    "\n",
    "Using the gradient function given to us in the Lecture 003 slides, the gradient ascent is calculted. w  is intialized as a column vector of zeroes and the hypothesis fucntion is used to find h(x). The gradient of the function is obtained by the dot product of X train data transposed and the difference between the plotted X data (h(x)) and y. this is then dividded by the number of training examples. w is then updated to reflect the difference in the learnigrate and the gradient. A list of every 100th likelihood value is kept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Write the gradient ascent function \n",
    "\n",
    "def Logistic_Regresion_Gradient_Ascent(X, y, learning_rate, num_iters):\n",
    "    # For every 100 iterations, store the log_likelihood for the current w\n",
    "    # Initializing log_likelihood to be an empty list \n",
    "    log_likelihood_values = []\n",
    "    \n",
    "    # Initialize w to be a zero vector of shape x_train.shape[1],1\n",
    "    w = np.zeros((X_train.shape[1], 1))\n",
    "#     w = w.reshape(X_train.shape[1],1)\n",
    "    # Initialize N to the number of training examples\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    y = y.reshape(y.shape[0], 1)# shapes y into a 426,1 matrix which helps with operations and calculations\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # update the w using formula \n",
    "        h = hypothesis(X, w)#get h(x) of the function from the hypothesis function implemented above \n",
    "        v = (1/N) * np.dot(X.T, h-y) #calculates the gradient from formula given  through gradient function\n",
    "        \n",
    "        w = w - (learning_rate*v) \n",
    "        \n",
    "        # append the log_likelihood values to the list for every 100 iterations\n",
    "        if (i%100 == 0):\n",
    "            Lw = log_likelihood(X, y, w) \n",
    "            \n",
    "            log_likelihood_values.append(Lw)\n",
    "    \n",
    "    return w, log_likelihood_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  The hyperparameters are set and the model is ran with th eregular X train and y train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3479181 ]\n",
      " [-0.26846821]\n",
      " [ 0.51657668]\n",
      " [-0.16993257]\n",
      " [-0.72022694]\n",
      " [-0.32745742]\n",
      " [ 3.00469581]\n",
      " [-1.07415832]\n",
      " [-1.71089927]\n",
      " [ 0.38429594]\n",
      " [ 0.19956861]\n",
      " [-3.74768423]\n",
      " [ 0.40337874]\n",
      " [-0.49274761]\n",
      " [-2.96966265]\n",
      " [-0.79891418]\n",
      " [ 1.10607911]\n",
      " [ 1.06976183]\n",
      " [-1.25387814]\n",
      " [ 0.59692961]\n",
      " [ 1.46187475]\n",
      " [-1.61670271]\n",
      " [-3.19542022]\n",
      " [-0.56756563]\n",
      " [-2.15085816]\n",
      " [-0.1957716 ]\n",
      " [ 0.33480951]\n",
      " [-2.46646051]\n",
      " [-1.89587629]\n",
      " [-2.15487182]\n",
      " [-1.46952252]]\n",
      "_____________________________________________________\n",
      "\n",
      "\n",
      "[-0.24640406747117044, -0.0727033462563158, -0.06415637856559564, -0.05999061973560711, -0.05736644586346294, -0.055507276189742605, -0.0540861152938868, -0.05293830956490462, -0.05197351157264211, -0.0511386777401165, -0.0504006213689146, -0.049737395297107544, -0.04913380559651238, -0.04857893250782669, -0.0480646814543665, -0.04758489397826201, -0.04713477933216217, -0.04671053817129714, -0.04630910588052647, -0.04592797295181272, -0.04556505646011527, -0.04521860630584931, -0.044887135648615746, -0.04456936850820357, -0.04426419976092123, -0.04397066422355882, -0.04368791249104024, -0.043415191854722805, -0.04315183108511344, -0.042897228183748436, -0.04265084043781644, -0.04241217627639893, -0.04218078854800168, -0.041956268928252514, -0.04173824323315273, -0.041526367463272554, -0.04132032444218997, -0.04111982094140732, -0.040924585206231906, -0.04073436481431705, -0.040548924811967556, -0.04036804608380945, -0.04019152391969666, -0.04001916674928031, -0.03985079501989046, -0.03968624019756208, -0.03952534387441089, -0.03936795696829267, -0.03921393900290558, -0.039063157458314145]\n"
     ]
    }
   ],
   "source": [
    "# Set the learning_rate\n",
    "learning_rate = 0.5\n",
    "# Set the num_iters\n",
    "num_iters = 5000\n",
    "# Run the Logistic_Regresion_Gradient_Ascent() and store the returned values\n",
    "w,log_likelihood_values = Logistic_Regresion_Gradient_Ascent(X_train, y_train, learning_rate, num_iters)\n",
    "\n",
    "print(w)\n",
    "print(\"_____________________________________________________\\n\\n\") \n",
    "print(log_likelihood_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######   Plotting Likelihood v/s Number of Iterations get a visual representaion of the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to plot Likelihood v/s Number of Iterations.\n",
    "iters = np.array(range(0,num_iters,100))\n",
    "plt.plot(iters,log_likelihood_values,'.-',color='green')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.title(\"Likelihood vs Number of Iterations.\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Evaluating your model\n",
    "\n",
    "Getting the predicted values of the X test dataset using the hypothsesis function and then evaluating the accuracy of the model by calculating the frequecy of slas positives and negatives and true positives and negatives. This is done by comparing thevalue of the y test data set and seeing whether it is positive (1) or negative(0) and then using the prdicted value to see whether it is larger than (positive) or smaller(negative) than 0.5. If they are the same then it is true, else ti is false. Afterwards the precision (The % of correctly predicted sample among all the samples that predicted 1), recall (The % of correctly predicted sample among all the samples that are actually 1) and f1 scale (A measure that combines precisionand recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hypothesis(...) to predict.\n",
    "    \n",
    "pred_y = hypothesis(X_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9767441860465116\n",
      "Recall:  0.9767441860465116\n",
      "F1:  0.9767441860465116\n",
      "Confusion Matrix: \n",
      "TP:  84  FN:  2  FP:  2  TN:  55\n"
     ]
    }
   ],
   "source": [
    "#using confusion matrix information and equations given in project 1 slides\n",
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "TN=0\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    # count TP,FP,FN,FP\n",
    "    if y_test[i] == 1: #test values are either 0 and 1, where 1 is positive and 0 is negative\n",
    "        if (pred_y[i] < 0.5) :#if the predicted value is less than 0.5 closer to 0 than to 1 then this is a false postive based on project 1 slides\n",
    "            FP+=1\n",
    "        else:\n",
    "            TP+=1\n",
    "    if y_test[i] == 0: \n",
    "        if (pred_y[i] < 0.5) :#if the predicted value is less than 0.5 closer to 0 than to 1 then this is a true negative based on project 1 slides\n",
    "            TN+=1\n",
    "        else:\n",
    "            FN+=1\n",
    "            \n",
    "# calculate precision, recall and f1\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "f1 = 2*((precision*recall) / (precision + recall))\n",
    "\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1: \",f1)\n",
    "print(\"Confusion Matrix: \")\n",
    "print(\"TP: \",TP,\" FN: \",FN,\" FP: \",FP,\" TN: \",TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
